{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Basics - Stop words, CMU WordList, WordNet, Tokenisation with POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *Stop words* are words which are filtered out before processing of natural language data. Stop words are generally the most common words in a language. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Extraction of the stop words from the text can be done with the help of the stopwords functionality that can be imported from nltk.corpus. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Stop words for languages other than English (such as German) can be extracted by specifying the target language wothin brackets. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'dass',\n",
       " 'daß',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'für',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'können',\n",
       " 'könnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'über',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'während',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of CMU Wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The *Carnegie Mellon University Pronouncing Dictionary* is an open-source machine-readable pronunciation dictionary for North American English that contains over 134,000 words and their pronunciations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We can evaluate the number of entries in the CMU WordList using the *cmudict.entries()* functionality. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entries=nltk.corpus.cmudict.entries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133737"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We can extract specific entries within a particular range of indices within the word list as follows. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abreu', ['AH0', 'B', 'R', 'UW1'])\n",
      "('abridge', ['AH0', 'B', 'R', 'IH1', 'JH'])\n",
      "('abridged', ['AH0', 'B', 'R', 'IH1', 'JH', 'D'])\n",
      "('abridgement', ['AH0', 'B', 'R', 'IH1', 'JH', 'M', 'AH0', 'N', 'T'])\n",
      "('abridges', ['AH0', 'B', 'R', 'IH1', 'JH', 'AH0', 'Z'])\n",
      "('abridging', ['AH0', 'B', 'R', 'IH1', 'JH', 'IH0', 'NG'])\n",
      "('abril', ['AH0', 'B', 'R', 'IH1', 'L'])\n",
      "('abroad', ['AH0', 'B', 'R', 'AO1', 'D'])\n",
      "('abrogate', ['AE1', 'B', 'R', 'AH0', 'G', 'EY2', 'T'])\n",
      "('abrogated', ['AE1', 'B', 'R', 'AH0', 'G', 'EY2', 'T', 'IH0', 'D'])\n",
      "('abrogating', ['AE1', 'B', 'R', 'AH0', 'G', 'EY2', 'T', 'IH0', 'NG'])\n",
      "('abrogation', ['AE2', 'B', 'R', 'AH0', 'G', 'EY1', 'SH', 'AH0', 'N'])\n",
      "('abrol', ['AH0', 'B', 'R', 'OW1', 'L'])\n",
      "('abron', ['AH0', 'B', 'R', 'AA1', 'N'])\n",
      "('abrupt', ['AH0', 'B', 'R', 'AH1', 'P', 'T'])\n",
      "('abruptly', ['AH0', 'B', 'R', 'AH1', 'P', 'T', 'L', 'IY0'])\n",
      "('abruptness', ['AH0', 'B', 'R', 'AH1', 'P', 'T', 'N', 'AH0', 'S'])\n",
      "('abrutyn', ['EY1', 'B', 'R', 'UW0', 'T', 'IH0', 'N'])\n",
      "('abruzzese', ['AA0', 'B', 'R', 'UW0', 'T', 'S', 'EY1', 'Z', 'IY0'])\n",
      "('abruzzo', ['AA0', 'B', 'R', 'UW1', 'Z', 'OW0'])\n",
      "('abs', ['EY1', 'B', 'IY1', 'EH1', 'S'])\n",
      "('abs', ['AE1', 'B', 'Z'])\n",
      "('absalom', ['AE1', 'B', 'S', 'AH0', 'L', 'AH0', 'M'])\n",
      "('absaraka', ['AE0', 'B', 'S', 'AA1', 'R', 'AH0', 'K', 'AH0'])\n",
      "('abscam', ['AE1', 'B', 'S', 'K', 'AE0', 'M'])\n",
      "('abscess', ['AE1', 'B', 'S', 'EH2', 'S'])\n",
      "('abscond', ['AE0', 'B', 'S', 'K', 'AA1', 'N', 'D'])\n",
      "('absconded', ['AE0', 'B', 'S', 'K', 'AA1', 'N', 'D', 'AH0', 'D'])\n",
      "('absconding', ['AE0', 'B', 'S', 'K', 'AA1', 'N', 'D', 'IH0', 'NG'])\n",
      "('absconds', ['AE0', 'B', 'S', 'K', 'AA1', 'N', 'D', 'Z'])\n",
      "('absecon', ['AE1', 'B', 'S', 'AH0', 'K', 'AO0', 'N'])\n",
      "('absence', ['AE1', 'B', 'S', 'AH0', 'N', 'S'])\n",
      "('absences', ['AE1', 'B', 'S', 'AH0', 'N', 'S', 'IH0', 'Z'])\n",
      "('absent', ['AE1', 'B', 'S', 'AH0', 'N', 'T'])\n",
      "('absentee', ['AE2', 'B', 'S', 'AH0', 'N', 'T', 'IY1'])\n",
      "('absenteeism', ['AE2', 'B', 'S', 'AH0', 'N', 'T', 'IY1', 'IH0', 'Z', 'AH0', 'M'])\n",
      "('absentees', ['AE2', 'B', 'S', 'AH0', 'N', 'T', 'IY1', 'Z'])\n",
      "('absentia', ['AE0', 'B', 'S', 'EH1', 'N', 'SH', 'AH0'])\n",
      "('absher', ['AE1', 'B', 'SH', 'ER0'])\n",
      "('abshier', ['AE1', 'B', 'SH', 'IY0', 'ER0'])\n",
      "('abshire', ['AE1', 'B', 'SH', 'AY2', 'R'])\n",
      "('absinthe', ['AE1', 'B', 'S', 'IH0', 'N', 'TH'])\n",
      "('abso', ['AE1', 'B', 'S', 'OW0'])\n",
      "('absolom', ['AE1', 'B', 'S', 'AH0', 'L', 'AH0', 'M'])\n",
      "('absolut', ['AE2', 'B', 'S', 'AH0', 'L', 'UW1', 'T'])\n",
      "('absolute', ['AE1', 'B', 'S', 'AH0', 'L', 'UW2', 'T'])\n",
      "('absolutely', ['AE2', 'B', 'S', 'AH0', 'L', 'UW1', 'T', 'L', 'IY0'])\n",
      "('absoluteness', ['AE1', 'B', 'S', 'AH0', 'L', 'UW2', 'T', 'N', 'AH0', 'S'])\n",
      "('absolutes', ['AE1', 'B', 'S', 'AH0', 'L', 'UW2', 'T', 'S'])\n",
      "('absolution', ['AE2', 'B', 'S', 'AH0', 'L', 'UW1', 'SH', 'AH0', 'N'])\n",
      "('absolutism', ['AE1', 'B', 'S', 'AH0', 'L', 'UW2', 'T', 'IH2', 'Z', 'AH0', 'M'])\n",
      "('absolutist', ['AE0', 'B', 'S', 'IH0', 'L', 'UW1', 'T', 'IH0', 'S', 'T'])\n",
      "(\"absolut's\", ['AE2', 'B', 'S', 'AH0', 'L', 'UW1', 'T', 'S'])\n",
      "('absolve', ['AH0', 'B', 'Z', 'AA1', 'L', 'V'])\n",
      "('absolve', ['AE0', 'B', 'Z', 'AA1', 'L', 'V'])\n",
      "('absolved', ['AH0', 'B', 'Z', 'AA1', 'L', 'V', 'D'])\n",
      "('absolved', ['AE0', 'B', 'Z', 'AA1', 'L', 'V', 'D'])\n",
      "('absolves', ['AH0', 'B', 'Z', 'AA1', 'L', 'V', 'Z'])\n",
      "('absolves', ['AE0', 'B', 'Z', 'AA1', 'L', 'V', 'Z'])\n",
      "('absolving', ['AH0', 'B', 'Z', 'AA1', 'L', 'V', 'IH0', 'NG'])\n",
      "('absolving', ['AE0', 'B', 'Z', 'AA1', 'L', 'V', 'IH0', 'NG'])\n",
      "('absorb', ['AH0', 'B', 'Z', 'AO1', 'R', 'B'])\n",
      "('absorbed', ['AH0', 'B', 'Z', 'AO1', 'R', 'B', 'D'])\n",
      "('absorbency', ['AH0', 'B', 'Z', 'AO1', 'R', 'B', 'AH0', 'N', 'S', 'IY0'])\n",
      "('absorbent', ['AH0', 'B', 'Z', 'AO1', 'R', 'B', 'AH0', 'N', 'T'])\n",
      "('absorber', ['AH0', 'B', 'Z', 'AO1', 'R', 'B', 'ER0'])\n",
      "('absorbers', ['AH0', 'B', 'Z', 'AO1', 'R', 'B', 'ER0', 'Z'])\n",
      "('absorbing', ['AH0', 'B', 'Z', 'AO1', 'R', 'B', 'IH0', 'NG'])\n",
      "('absorbs', ['AH0', 'B', 'Z', 'AO1', 'R', 'B', 'Z'])\n",
      "('absorption', ['AH0', 'B', 'Z', 'AO1', 'R', 'P', 'SH', 'AH0', 'N'])\n",
      "('absorption', ['AH0', 'B', 'S', 'AO1', 'R', 'P', 'SH', 'AH0', 'N'])\n",
      "('abstain', ['AH0', 'B', 'S', 'T', 'EY1', 'N'])\n",
      "('abstain', ['AE0', 'B', 'S', 'T', 'EY1', 'N'])\n",
      "('abstained', ['AH0', 'B', 'S', 'T', 'EY1', 'N', 'D'])\n",
      "('abstained', ['AE0', 'B', 'S', 'T', 'EY1', 'N', 'D'])\n",
      "('abstaining', ['AH0', 'B', 'S', 'T', 'EY1', 'N', 'IH0', 'NG'])\n",
      "('abstaining', ['AE0', 'B', 'S', 'T', 'EY1', 'N', 'IH0', 'NG'])\n",
      "('abstention', ['AH0', 'B', 'S', 'T', 'EH1', 'N', 'CH', 'AH0', 'N'])\n",
      "('abstention', ['AE0', 'B', 'S', 'T', 'EH1', 'N', 'CH', 'AH0', 'N'])\n",
      "('abstentions', ['AH0', 'B', 'S', 'T', 'EH1', 'N', 'CH', 'AH0', 'N', 'Z'])\n",
      "('abstentions', ['AE0', 'B', 'S', 'T', 'EH1', 'N', 'CH', 'AH0', 'N', 'Z'])\n",
      "('abstinence', ['AE1', 'B', 'S', 'T', 'AH0', 'N', 'AH0', 'N', 'S'])\n",
      "('abstinent', ['AE1', 'B', 'S', 'T', 'AH0', 'N', 'AH0', 'N', 'T'])\n",
      "('abston', ['AE1', 'B', 'S', 'T', 'AH0', 'N'])\n",
      "('abstract', ['AE0', 'B', 'S', 'T', 'R', 'AE1', 'K', 'T'])\n",
      "('abstract', ['AE1', 'B', 'S', 'T', 'R', 'AE2', 'K', 'T'])\n",
      "('abstracted', ['AE1', 'B', 'S', 'T', 'R', 'AE2', 'K', 'T', 'IH0', 'D'])\n",
      "('abstraction', ['AE0', 'B', 'S', 'T', 'R', 'AE1', 'K', 'SH', 'AH0', 'N'])\n",
      "('abstractions', ['AE0', 'B', 'S', 'T', 'R', 'AE1', 'K', 'SH', 'AH0', 'N', 'Z'])\n",
      "('abstracts', ['AE1', 'B', 'S', 'T', 'R', 'AE0', 'K', 'T', 'S'])\n",
      "('abstruse', ['AH0', 'B', 'S', 'T', 'R', 'UW1', 'S'])\n",
      "('absurd', ['AH0', 'B', 'S', 'ER1', 'D'])\n",
      "('absurdist', ['AH0', 'B', 'S', 'ER1', 'D', 'IH0', 'S', 'T'])\n",
      "('absurdities', ['AH0', 'B', 'S', 'ER1', 'D', 'AH0', 'T', 'IY0', 'Z'])\n",
      "('absurdity', ['AH0', 'B', 'S', 'ER1', 'D', 'AH0', 'T', 'IY0'])\n",
      "('absurdly', ['AH0', 'B', 'S', 'ER1', 'D', 'L', 'IY0'])\n",
      "('abt', ['AE1', 'B', 'T'])\n",
      "('abt', ['EY1', 'B', 'IY1', 'T', 'IY1'])\n",
      "('abts', ['AE1', 'B', 'T', 'S'])\n",
      "('abts', ['EY1', 'B', 'IY1', 'T', 'IY1', 'Z'])\n"
     ]
    }
   ],
   "source": [
    "for entry in entries[300:400]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *WordNet* is a lexical database of semantic relations between words in more than 200 languages. WordNet links words into semantic relations including synonyms, hyponyms, and meronyms. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The synonyms are grouped into synsets with short definitions and usage examples.\n",
    "We get an id of subsets using the *wn.synsets()* functionality. It has an optional pos argument which lets you constrain the part of speech of the word. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('car')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** A synset is identified with a 3-part name of the form: *word.pos.nn*. We can extract all those synsets that have a particular synset identifier using the *lemma_names()* functionality. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'elevator_car']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.04').lemma_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts=\"\"\"Benedict Timothy Carlton Cumberbatch CBE (born 19 July 1976) is an English actor. A graduate of the Victoria University of Manchester, he continued his training at the London Academy of Music and Dramatic Art, obtaining a Master of Arts in Classical Acting. He first performed at the Open Air Theatre, Regent's Park in Shakespearean productions and made his West End debut in Richard Eyre's revival of Hedda Gabler in 2005. Since then, he has starred in the Royal National Theatre productions After the Dance (2010) and Frankenstein (2011). In 2015, he played William Shakespeare's Hamlet at the Barbican Theatre.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Tokenisation procedure follows the sequence of *sentence tokenisation* followed by *word tokenisation* ie, the text is firstly tokenised into sentences using the period delimiter and then they are in turn tokenised into words using the word delimiter. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Next the extracted words are provided with *parts-of-speech* tag. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('t', 'NN')]\n",
      "[('T', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('m', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('C', 'SYM')]\n",
      "[('a', 'DT')]\n",
      "[('r', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('C', 'SYM')]\n",
      "[('u', 'NN')]\n",
      "[('m', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('t', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('h', 'NN')]\n",
      "[('C', 'SYM')]\n",
      "[('B', 'NN')]\n",
      "[('E', 'NN')]\n",
      "[('(', '(')]\n",
      "[('b', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('1', 'CD')]\n",
      "[('9', 'CD')]\n",
      "[('J', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('1', 'CD')]\n",
      "[('9', 'CD')]\n",
      "[('7', 'CD')]\n",
      "[('6', 'CD')]\n",
      "[(')', ')')]\n",
      "[('i', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('n', 'NN')]\n",
      "[('E', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('c', 'NNS')]\n",
      "[('t', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('.', '.')]\n",
      "[('A', 'DT')]\n",
      "[('g', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('d', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('t', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('V', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('t', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('U', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('v', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('M', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('n', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[(',', ',')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('t', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('L', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('A', 'DT')]\n",
      "[('c', 'NNS')]\n",
      "[('a', 'DT')]\n",
      "[('d', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('m', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('M', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('a', 'DT')]\n",
      "[('n', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('D', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('m', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('t', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('A', 'DT')]\n",
      "[('r', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[(',', ',')]\n",
      "[('o', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('M', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('A', 'DT')]\n",
      "[('r', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('C', 'SYM')]\n",
      "[('l', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('s', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('a', 'DT')]\n",
      "[('l', 'NN')]\n",
      "[('A', 'DT')]\n",
      "[('c', 'NNS')]\n",
      "[('t', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('.', '.')]\n",
      "[('H', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('m', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('t', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('O', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('A', 'DT')]\n",
      "[('i', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('T', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('t', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[(',', ',')]\n",
      "[('R', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[(\"'\", \"''\")]\n",
      "[('s', 'NN')]\n",
      "[('P', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('r', 'NN')]\n",
      "[('k', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('S', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('k', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('r', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('n', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('t', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('n', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('m', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('d', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('W', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('E', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('R', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('r', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('E', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[(\"'\", \"''\")]\n",
      "[('s', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('v', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('v', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('l', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('H', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('G', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('b', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('2', 'CD')]\n",
      "[('0', 'CD')]\n",
      "[('0', 'CD')]\n",
      "[('5', 'CD')]\n",
      "[('.', '.')]\n",
      "[('S', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('e', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[(',', ',')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('s', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('r', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('R', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('l', 'NN')]\n",
      "[('N', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('t', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('l', 'NN')]\n",
      "[('T', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('t', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('t', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('A', 'DT')]\n",
      "[('f', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('D', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('n', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('e', 'NN')]\n",
      "[('(', '(')]\n",
      "[('2', 'CD')]\n",
      "[('0', 'CD')]\n",
      "[('1', 'CD')]\n",
      "[('0', 'CD')]\n",
      "[(')', ')')]\n",
      "[('a', 'DT')]\n",
      "[('n', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('F', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('n', 'NN')]\n",
      "[('k', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('(', '(')]\n",
      "[('2', 'CD')]\n",
      "[('0', 'CD')]\n",
      "[('1', 'CD')]\n",
      "[('1', 'CD')]\n",
      "[(')', ')')]\n",
      "[('.', '.')]\n",
      "[('I', 'PRP')]\n",
      "[('n', 'NN')]\n",
      "[('2', 'CD')]\n",
      "[('0', 'CD')]\n",
      "[('1', 'CD')]\n",
      "[('5', 'CD')]\n",
      "[(',', ',')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('y', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('W', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('m', 'NN')]\n",
      "[('S', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('k', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('r', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[(\"'\", \"''\")]\n",
      "[('s', 'NN')]\n",
      "[('H', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('m', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('t', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('B', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('r', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('a', 'DT')]\n",
      "[('n', 'NN')]\n",
      "[('T', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('t', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    sentences=nltk.sent_tokenize(text) #Tokenising into sentences\n",
    "    for sentence in sentences:\n",
    "        words=nltk.word_tokenize(sentence) #Tokenising the words in the given sentence\n",
    "        tagged_words=nltk.pos_tag(words) #Assigning POS\n",
    "        print(tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Twitter aware tokenizer. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The *TweetTokenizer* functionaity imported from *nltk.tokenize* can be used for effectively executing the tokenisation of text in the form of tweets. ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text='THANK YOU TOLEDO, OHIO :('"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twtkn=TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** It is capable of properly classifying and effectively tokenising even keyboard emoticons that are part of the text. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THANK', 'YOU', 'TOLEDO', ',', 'OHIO', ':(']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twtkn.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Analysis using Brown corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We extract the words that are part of the specified category. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_text=brown.words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n"
     ]
    }
   ],
   "source": [
    "print(news_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** For the purpose of normalisation and effective comparison, we reduce all the words to the lower case. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=nltk.FreqDist(w.lower() for w in news_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Next we try to evaluate the number of occurences of each modal word separately within the list of words. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modals=['can','could','may','might','will','must']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can: 94 could: 87 may: 93 might: 38 will: 389 must: 53 "
     ]
    }
   ],
   "source": [
    "for a in modals:\n",
    "    print(a+':',f[a],end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 13112 samples and 100554 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
